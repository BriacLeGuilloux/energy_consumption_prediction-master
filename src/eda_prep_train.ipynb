{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy consumption prediction using LSTM/GRU in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we tackle a time series forecasting task using GRU and LSTM models implemented with PyTorch. Our objective is to predict the next hour’s energy consumption based on historical usage data. We use the Hourly Energy Consumption dataset, which provides hourly power usage across various U.S. regions.\n",
    "\n",
    "We compare the performance of GRU and LSTM by training both models on past data and evaluating them on a separate test set. The workflow includes feature engineering, data preprocessing, model definition, training, and evaluation. Common Python libraries are used throughout the process to support data analysis and modeling.\n",
    "\n",
    "Source : [Kaggle](https://www.kaggle.com/robikscube/hourly-energy-consumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU/LSTM cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Long Short-Term Memory networks (LSTMs) have great memories and can remember information which the vanilla RNN is unable to!\n",
    "\n",
    "* The Gated Recurrent Unit (GRU) is the younger sibling of the more popular Long Short-Term Memory (LSTM) network, and also a type of Recurrent Neural Network (RNN). Just like its sibling, GRUs are able to effectively retain long-term dependencies in sequential data. And additionally, they can address the “short-term memory” issue plaguing vanilla RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Third-party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local module\n",
    "from fct import move_sliding_window\n",
    "from fct import GRUNet, LSTMNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of parameters\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accès aux variables\n",
    "label_col_index = config[\"label_col_index\"]\n",
    "inputs_cols_indices = config[\"inputs_cols_indices\"]\n",
    "window_size = config[\"window_size\"]\n",
    "num_files_for_dataset = config[\"num_files_for_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_raw_path = Path.cwd().parent / \"data\" / \"raw\"\n",
    "data_processed_path = Path.cwd().parent / \"data\" / \"processed\"\n",
    "model_path = Path.cwd().parent / \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>DEOK_MW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-12-31 01:00:00</td>\n",
       "      <td>2945.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-12-31 02:00:00</td>\n",
       "      <td>2868.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-12-31 03:00:00</td>\n",
       "      <td>2812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-31 04:00:00</td>\n",
       "      <td>2812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-31 05:00:00</td>\n",
       "      <td>2860.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Datetime  DEOK_MW\n",
       "0  2012-12-31 01:00:00   2945.0\n",
       "1  2012-12-31 02:00:00   2868.0\n",
       "2  2012-12-31 03:00:00   2812.0\n",
       "3  2012-12-31 04:00:00   2812.0\n",
       "4  2012-12-31 05:00:00   2860.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try read a single file\n",
    "pd.read_csv(os.path.join(data_raw_path, \"DEOK_hourly.csv\")).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of **12** *.csv* files containing hourly energy trend data (note that *'est_hourly.parquet'* and *'pjm_hourly_est.csv'* are excluded). Our next step consists of reading these files and preprocessing the data in the following order:\n",
    "\n",
    "- Extract and generalize the time features for each time step:\n",
    "    - Hour of the day (0–23)\n",
    "    - Day of the week (1–7)\n",
    "    - Month (1–12)\n",
    "    - Day of the year (1–365)\n",
    "\n",
    "- Scale the data to values between 0 and 1:\n",
    "    - Scaling helps algorithms perform better and converge faster by putting features on a comparable scale and closer to a normal distribution.\n",
    "    - This scaling preserves the original distribution’s shape and maintains the impact of outliers.\n",
    "\n",
    "- Organize the data into sequences to serve as model inputs and prepare the corresponding labels:\n",
    "    - The **sequence length** or **window_size** defines how many historical data points the model will use to predict the future.\n",
    "    - The label corresponds to the data point immediately following the last point in the input sequence.\n",
    "\n",
    "- Finally, split the inputs and labels into training and test sets for model development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training instances by moving sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate files to build the training set\n",
    "To speed things up, I will only be using `num_files_for_dataset` .csv files for creating my dataset. Feel free to run it yourself with the entire dataset if you have the time and computing capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d248365ecd934a03bbd80305885625b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AEP_hourly.csv ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121183, 90, 5) (121183, 1)\n",
      "Processing COMED_hourly.csv ...\n",
      "(66407, 90, 5) (66407, 1)\n",
      "Processing DAYTON_hourly.csv ...\n",
      "(121185, 90, 5) (121185, 1)\n",
      "Processing DEOK_hourly.csv ...\n",
      "(57649, 90, 5) (57649, 1)\n",
      "Processing DOM_hourly.csv ...\n",
      "(116099, 90, 5) (116099, 1)\n"
     ]
    }
   ],
   "source": [
    "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
    "label_scalers = {}\n",
    "\n",
    "train_x = []\n",
    "test_x = {}\n",
    "test_y = {}\n",
    "\n",
    "# Skipping the files we're not using\n",
    "processing_files = [\n",
    "    file for file in os.listdir(data_raw_path) if os.path.splitext(file)[1] == \".csv\"\n",
    "]\n",
    "\n",
    "for file in tqdm_notebook(processing_files[:num_files_for_dataset]):\n",
    "    print(f\"Processing {file} ...\")\n",
    "    # Store csv file in a Pandas DataFrame\n",
    "    df = pd.read_csv(os.path.join(data_raw_path, file), parse_dates=[\"Datetime\"])\n",
    "\n",
    "    # Processing the time data into suitable input formats\n",
    "    df = df.assign(\n",
    "        hour=df[\"Datetime\"].dt.hour,\n",
    "        dayofweek=df[\"Datetime\"].dt.dayofweek,\n",
    "        month=df[\"Datetime\"].dt.month,\n",
    "        dayofyear=df[\"Datetime\"].dt.dayofyear,\n",
    "    )\n",
    "    df = df.sort_values(\"Datetime\").drop(\"Datetime\", axis=1)\n",
    "\n",
    "    # Scaling the input data\n",
    "    sc = MinMaxScaler()\n",
    "    label_sc = MinMaxScaler()\n",
    "    data = sc.fit_transform(df.values)\n",
    "    \n",
    "\n",
    "    # Obtaining the scaler for the labels(usage data) so that output can be\n",
    "    # re-scaled to actual value during evaluation\n",
    "    label_sc.fit(df.iloc[:, label_col_index].values.reshape(-1, 1))\n",
    "    label_scalers[file] = label_sc\n",
    "\n",
    "    # Move the window\n",
    "    inputs, labels = move_sliding_window(\n",
    "        data,\n",
    "        window_size,\n",
    "        inputs_cols_indices=inputs_cols_indices,\n",
    "        label_col_index=label_col_index,\n",
    "    )\n",
    "    \n",
    "    # Redure the precision of data\n",
    "    data = data.astype(np.float32)\n",
    "    inputs = inputs.astype(np.float32)\n",
    "    labels = labels.astype(np.float32)\n",
    "\n",
    "    # CONCAT created instances from all .csv files.\n",
    "    # Split data into train/test portions and combining all data from different files into a single array\n",
    "    test_portion = int(0.1 * len(inputs))\n",
    "    if len(train_x) == 0:  # first iteration\n",
    "        train_x = inputs[:-test_portion]\n",
    "        train_y = labels[:-test_portion]\n",
    "    else:\n",
    "        train_x = np.concatenate((train_x, inputs[:-test_portion]))\n",
    "        train_y = np.concatenate((train_y, labels[:-test_portion]))\n",
    "    test_x[file] = inputs[-test_portion:]\n",
    "    test_y[file] = labels[-test_portion:]\n",
    "    \n",
    "    # Remove temporary variables\n",
    "    del df, data, inputs, labels\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we made?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((434274, 90, 5), (5764, 90, 5))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, test_x[\"DEOK_hourly.csv\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch data loaders/generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the speed of our training, we can process the data in batches so that the model does not need to update its weights as frequently. The `TensorDataset` and `DataLoader` classes are useful for splitting our data into batches and shuffling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config[\"batch_size\"]\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "\n",
    "# Drop the last incomplete batch\n",
    "train_loader = DataLoader(\n",
    "    train_data, shuffle=True, batch_size=batch_size, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: (434274, 90, 5), Batch Size: 500, # of iterations per epoch: 868\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Train Size: {train_x.shape}, Batch Size: {batch_size}, # of iterations per epoch: {int(train_x.shape[0]/batch_size)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release some memory\n",
    "del train_x, train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if we have any GPUs to speed up our training time by many folds. If you’re using \"https://colab.research.google.com/\" with GPU to run this code, the training time will be significantly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll be defining the structure of the GRU and LSTM models. Both models have the same structure, with the only difference being the **recurrent layer** (GRU/LSTM) and the initializing of the hidden state. The hidden state for the LSTM is a tuple containing both the **cell state** and the **hidden state**, whereas the **GRU only has a single hidden state**. \n",
    "Please refer to official PyTorch documentation to get familiar with GRU and LSTM interfaces in PyTorch:\n",
    "\n",
    "- https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
    "- https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_loader,\n",
    "    learn_rate,\n",
    "    hidden_dim=256,\n",
    "    n_layers=2,\n",
    "    n_epochs=5,\n",
    "    model_type=\"GRU\",\n",
    "    print_every=100,\n",
    "):\n",
    "\n",
    "    input_dim = next(iter(train_loader))[0].shape[2]  # 5\n",
    "\n",
    "    # Batch generator (train_data, train_label)\n",
    "    # print(next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape) # torch.Size([1024, 90, 5]) torch.Size([1024, 1])\n",
    "\n",
    "    output_dim = 1\n",
    "\n",
    "    # Instantiating the models\n",
    "    if model_type == \"GRU\":\n",
    "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    else:\n",
    "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "    model.to(device)\n",
    "\n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "    model.train()\n",
    "    print(\"Starting Training of {} model\".format(model_type))\n",
    "    epoch_times = []\n",
    "\n",
    "    # Start training loop\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        start_time = time.process_time()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.0\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            if model_type == \"GRU\":\n",
    "                h = h.data\n",
    "            # Unpcak both h_0 and c_0\n",
    "            elif model_type == \"LSTM\":\n",
    "                h = tuple([e.data for e in h])\n",
    "\n",
    "            # Set the gradients to zero before starting to do backpropragation because\n",
    "            # PyTorch accumulates the gradients on subsequent backward passes\n",
    "            model.zero_grad()\n",
    "\n",
    "            out, h = model(x.to(device).float(), h)\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "\n",
    "            # Perform backpropragation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            if counter % print_every == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch} - Step: {counter}/{len(train_loader)} - Average Loss for Epoch: {avg_loss/counter}\"\n",
    "                )\n",
    "        current_time = time.process_time()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{n_epochs} Done, Total Loss: {avg_loss/len(train_loader)}\"\n",
    "        )\n",
    "\n",
    "        print(f\"Time Elapsed for Epoch: {current_time-start_time} seconds\")\n",
    "\n",
    "        epoch_times.append(current_time - start_time)\n",
    "\n",
    "    print(f\"Total Training Time: {sum(epoch_times)} seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 1 - Step: 100/868 - Average Loss for Epoch: 0.025420364928431808\n",
      "Epoch 1 - Step: 200/868 - Average Loss for Epoch: 0.013931295262300409\n",
      "Epoch 1 - Step: 300/868 - Average Loss for Epoch: 0.00988527845707722\n",
      "Epoch 1 - Step: 400/868 - Average Loss for Epoch: 0.007751718809595331\n",
      "Epoch 1 - Step: 500/868 - Average Loss for Epoch: 0.006418049654108473\n",
      "Epoch 1 - Step: 600/868 - Average Loss for Epoch: 0.0054986996486938245\n",
      "Epoch 1 - Step: 700/868 - Average Loss for Epoch: 0.004825032595212438\n",
      "Epoch 1 - Step: 800/868 - Average Loss for Epoch: 0.004310546857595909\n",
      "Epoch 1/5 Done, Total Loss: 0.004023309500712563\n",
      "Time Elapsed for Epoch: 1023.469607352 seconds\n",
      "Epoch 2 - Step: 100/868 - Average Loss for Epoch: 0.0006136846003937535\n",
      "Epoch 2 - Step: 200/868 - Average Loss for Epoch: 0.0005840548567357473\n",
      "Epoch 2 - Step: 300/868 - Average Loss for Epoch: 0.000565221977303736\n",
      "Epoch 2 - Step: 400/868 - Average Loss for Epoch: 0.0005450130865210667\n",
      "Epoch 2 - Step: 500/868 - Average Loss for Epoch: 0.0005253375316970051\n",
      "Epoch 2 - Step: 600/868 - Average Loss for Epoch: 0.0005082281986445499\n",
      "Epoch 2 - Step: 700/868 - Average Loss for Epoch: 0.0004923311852949805\n",
      "Epoch 2 - Step: 800/868 - Average Loss for Epoch: 0.000480503695725929\n",
      "Epoch 2/5 Done, Total Loss: 0.00047277064326332993\n",
      "Time Elapsed for Epoch: 947.1719762500002 seconds\n",
      "Epoch 3 - Step: 100/868 - Average Loss for Epoch: 0.00038862646382767705\n",
      "Epoch 3 - Step: 200/868 - Average Loss for Epoch: 0.0003712795097089838\n",
      "Epoch 3 - Step: 300/868 - Average Loss for Epoch: 0.00036109228628144287\n",
      "Epoch 3 - Step: 400/868 - Average Loss for Epoch: 0.0003547471590718487\n",
      "Epoch 3 - Step: 500/868 - Average Loss for Epoch: 0.00034844743693247435\n",
      "Epoch 3 - Step: 600/868 - Average Loss for Epoch: 0.00034119103865426346\n",
      "Epoch 3 - Step: 700/868 - Average Loss for Epoch: 0.0003358568919065874\n",
      "Epoch 3 - Step: 800/868 - Average Loss for Epoch: 0.00033135662983113434\n",
      "Epoch 3/5 Done, Total Loss: 0.000328371726316593\n",
      "Time Elapsed for Epoch: 1106.192532533 seconds\n",
      "Epoch 4 - Step: 100/868 - Average Loss for Epoch: 0.0003086358730797656\n",
      "Epoch 4 - Step: 200/868 - Average Loss for Epoch: 0.0003015361559664598\n",
      "Epoch 4 - Step: 300/868 - Average Loss for Epoch: 0.00029263154147580886\n",
      "Epoch 4 - Step: 400/868 - Average Loss for Epoch: 0.00028545064447826007\n",
      "Epoch 4 - Step: 500/868 - Average Loss for Epoch: 0.00028014364960836244\n",
      "Epoch 4 - Step: 600/868 - Average Loss for Epoch: 0.0002761239971005125\n",
      "Epoch 4 - Step: 700/868 - Average Loss for Epoch: 0.00027169373364553656\n",
      "Epoch 4 - Step: 800/868 - Average Loss for Epoch: 0.00026854427762373233\n",
      "Epoch 4/5 Done, Total Loss: 0.0002672801854186541\n",
      "Time Elapsed for Epoch: 1133.500404158 seconds\n",
      "Epoch 5 - Step: 100/868 - Average Loss for Epoch: 0.00023764566314639522\n",
      "Epoch 5 - Step: 200/868 - Average Loss for Epoch: 0.0002380497167177964\n",
      "Epoch 5 - Step: 300/868 - Average Loss for Epoch: 0.0002479924345000957\n",
      "Epoch 5 - Step: 400/868 - Average Loss for Epoch: 0.00024379621972911992\n",
      "Epoch 5 - Step: 500/868 - Average Loss for Epoch: 0.0002430632114992477\n",
      "Epoch 5 - Step: 600/868 - Average Loss for Epoch: 0.00023948575219643922\n",
      "Epoch 5 - Step: 700/868 - Average Loss for Epoch: 0.00023846525757107885\n",
      "Epoch 5 - Step: 800/868 - Average Loss for Epoch: 0.00023590173132106428\n",
      "Epoch 5/5 Done, Total Loss: 0.00023565620326714413\n",
      "Time Elapsed for Epoch: 1132.4335466620005 seconds\n",
      "Total Training Time: 5342.768066955001 seconds\n"
     ]
    }
   ],
   "source": [
    "# seq_len = 90  # (timestamps)\n",
    "# Paramètres pour l'entraînement\n",
    "n_hidden = config[\"n_hidden\"]\n",
    "n_layers = config[\"n_layers\"]\n",
    "n_epochs = config[\"n_epochs\"]\n",
    "print_every = config[\"print_every\"]\n",
    "lr = config[\"lr\"]\n",
    "\n",
    "gru_model = train(\n",
    "    train_loader,\n",
    "    learn_rate=lr,\n",
    "    hidden_dim=n_hidden,\n",
    "    n_layers=n_layers,\n",
    "    n_epochs=n_epochs,\n",
    "    model_type=\"GRU\",\n",
    "    print_every=print_every,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gru_model.state_dict(), os.path.join(model_path, \"gru_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Save an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of LSTM model\n",
      "Epoch 1 - Step: 100/868 - Average Loss for Epoch: 0.040653518978506324\n",
      "Epoch 1 - Step: 200/868 - Average Loss for Epoch: 0.02273187775281258\n",
      "Epoch 1 - Step: 300/868 - Average Loss for Epoch: 0.01625208417031293\n",
      "Epoch 1 - Step: 400/868 - Average Loss for Epoch: 0.01276947476726491\n",
      "Epoch 1 - Step: 500/868 - Average Loss for Epoch: 0.010554868601961061\n",
      "Epoch 1 - Step: 600/868 - Average Loss for Epoch: 0.009018121115902129\n",
      "Epoch 1 - Step: 700/868 - Average Loss for Epoch: 0.007883682757466367\n",
      "Epoch 1 - Step: 800/868 - Average Loss for Epoch: 0.0070186707939865305\n",
      "Epoch 1/5 Done, Total Loss: 0.0065334923662624025\n",
      "Time Elapsed for Epoch: 908.0744691480004 seconds\n",
      "Epoch 2 - Step: 100/868 - Average Loss for Epoch: 0.0007986272289417684\n",
      "Epoch 2 - Step: 200/868 - Average Loss for Epoch: 0.0007659769992460496\n",
      "Epoch 2 - Step: 300/868 - Average Loss for Epoch: 0.000732149876615343\n",
      "Epoch 2 - Step: 400/868 - Average Loss for Epoch: 0.0007022703820257448\n",
      "Epoch 2 - Step: 500/868 - Average Loss for Epoch: 0.0006746971827815287\n",
      "Epoch 2 - Step: 600/868 - Average Loss for Epoch: 0.0006547932569810655\n",
      "Epoch 2 - Step: 700/868 - Average Loss for Epoch: 0.0006294446623983926\n",
      "Epoch 2 - Step: 800/868 - Average Loss for Epoch: 0.0006093028260511347\n",
      "Epoch 2/5 Done, Total Loss: 0.000595930700546049\n",
      "Time Elapsed for Epoch: 994.0323193510003 seconds\n",
      "Epoch 3 - Step: 100/868 - Average Loss for Epoch: 0.00042864986724453044\n",
      "Epoch 3 - Step: 200/868 - Average Loss for Epoch: 0.0004331359112984501\n",
      "Epoch 3 - Step: 300/868 - Average Loss for Epoch: 0.00041998069810991485\n",
      "Epoch 3 - Step: 400/868 - Average Loss for Epoch: 0.00040994523638801186\n",
      "Epoch 3 - Step: 500/868 - Average Loss for Epoch: 0.0004013470850768499\n",
      "Epoch 3 - Step: 600/868 - Average Loss for Epoch: 0.00039674488793631707\n",
      "Epoch 3 - Step: 700/868 - Average Loss for Epoch: 0.0003902317038487776\n",
      "Epoch 3 - Step: 800/868 - Average Loss for Epoch: 0.000386634931928711\n",
      "Epoch 3/5 Done, Total Loss: 0.0003823400172646359\n",
      "Time Elapsed for Epoch: 923.3598091290005 seconds\n",
      "Epoch 4 - Step: 100/868 - Average Loss for Epoch: 0.00033414115227060393\n",
      "Epoch 4 - Step: 200/868 - Average Loss for Epoch: 0.00033015256849466823\n",
      "Epoch 4 - Step: 300/868 - Average Loss for Epoch: 0.0003271891883923672\n",
      "Epoch 4 - Step: 400/868 - Average Loss for Epoch: 0.00032285424415022133\n",
      "Epoch 4 - Step: 500/868 - Average Loss for Epoch: 0.000324562727677403\n",
      "Epoch 4 - Step: 600/868 - Average Loss for Epoch: 0.0003196987896808423\n",
      "Epoch 4 - Step: 700/868 - Average Loss for Epoch: 0.0003180725501650678\n",
      "Epoch 4 - Step: 800/868 - Average Loss for Epoch: 0.00031464052230148807\n",
      "Epoch 4/5 Done, Total Loss: 0.00031238050926769097\n",
      "Time Elapsed for Epoch: 922.2152595369998 seconds\n",
      "Epoch 5 - Step: 100/868 - Average Loss for Epoch: 0.0002797988150268793\n",
      "Epoch 5 - Step: 200/868 - Average Loss for Epoch: 0.0002833295687742066\n",
      "Epoch 5 - Step: 300/868 - Average Loss for Epoch: 0.00028502943911007606\n",
      "Epoch 5 - Step: 400/868 - Average Loss for Epoch: 0.00028402147108863574\n",
      "Epoch 5 - Step: 500/868 - Average Loss for Epoch: 0.0002844216960656922\n",
      "Epoch 5 - Step: 600/868 - Average Loss for Epoch: 0.0002824051265391366\n",
      "Epoch 5 - Step: 700/868 - Average Loss for Epoch: 0.00027925321799037714\n",
      "Epoch 5 - Step: 800/868 - Average Loss for Epoch: 0.000277354461595678\n",
      "Epoch 5/5 Done, Total Loss: 0.00027716246689455195\n",
      "Time Elapsed for Epoch: 919.8409336829991 seconds\n",
      "Total Training Time: 4667.522790848 seconds\n"
     ]
    }
   ],
   "source": [
    "lstm_model = train(\n",
    "    train_loader,\n",
    "    learn_rate=lr,\n",
    "    hidden_dim=n_hidden,\n",
    "    n_layers=n_layers,\n",
    "    n_epochs=n_epochs,\n",
    "    model_type=\"LSTM\",\n",
    "    print_every=print_every,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model.state_dict(), os.path.join(model_path, \"lstm_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the training time of both models, the GRU model is the clear winner in terms of speed, as we have mentioned earlier. The GRU finished 5 training epochs faster than the LSTM model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy-consumption-prediction-master-YcCQwa7r-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
